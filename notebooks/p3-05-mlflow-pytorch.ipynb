{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225b4513-9c5c-4a26-92f7-a8fde12373fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Adjust to your project's structure\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebbbf6b-7a6e-4adc-9542-a6ab2b8d59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLFlow tracking URI (local or server-based)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")  # Change if using a centralized server\n",
    "\n",
    "# Define the experiment name\n",
    "mlflow.set_experiment(\"MVP Prediction NN\")\n",
    "\n",
    "mlflow.set_tag(\"developer\", \"christophe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "539ebad6-26fa-4517-adcf-5ff79a4d2ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: torch.Size([206, 24]) torch.Size([206])\n",
      "Validation set: torch.Size([44, 24]) torch.Size([44])\n",
      "Test set: torch.Size([45, 24]) torch.Size([45])\n"
     ]
    }
   ],
   "source": [
    "# Load your cleaned dataset\n",
    "data_path = \"/Users/cb/src/nba_mvp_ml/data/processed/by_season/fully_merged/final_stacked_data.csv\"\n",
    "\n",
    "_X, _y = load_and_preprocess_data(data_path, remove_excess_features=True) # X will be normalized\n",
    "\n",
    "\n",
    "# Example input data\n",
    "np.random.seed(42)\n",
    "X =_X.to_numpy().astype(np.float32)\n",
    "y = _y.to_numpy().astype(np.int64)  # Binary labels\n",
    "\n",
    "# Determine sizes for train, validation, and test splits\n",
    "train_size = int(0.7 * len(X))  # 70% for training\n",
    "val_size = int(0.15 * len(X))   # 15% for validation\n",
    "test_size = len(X) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "# Split the datase\n",
    "X_train = torch.tensor(X[:train_size])\n",
    "y_train = torch.tensor(y[:train_size])\n",
    "\n",
    "X_val= torch.tensor(X[train_size:train_size + val_size])\n",
    "y_val= torch.tensor(y[train_size:train_size + val_size])\n",
    "\n",
    "X_test = torch.tensor(X[train_size + val_size:])\n",
    "y_test = torch.tensor(y[train_size + val_size:])\n",
    "\n",
    "_y_test = _y[train_size + val_size:]\n",
    "\n",
    "# Check the shapes of each split\n",
    "print(\"Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7524d4-b75b-4fd1-bfc6-d843699be433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 24\n",
    "hidden_size = 64\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleMLP(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16d94e08-db76-4720-be73-ea73f9cc08c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6965\n",
      "Validation Loss: 0.6490, Accuracy: 70.45%\n",
      "Epoch [2/20], Loss: 0.6409\n",
      "Validation Loss: 0.6010, Accuracy: 79.55%\n",
      "Epoch [3/20], Loss: 0.5935\n",
      "Validation Loss: 0.5623, Accuracy: 84.09%\n",
      "Epoch [4/20], Loss: 0.5598\n",
      "Validation Loss: 0.5320, Accuracy: 86.36%\n",
      "Epoch [5/20], Loss: 0.5288\n",
      "Validation Loss: 0.5091, Accuracy: 86.36%\n",
      "Epoch [6/20], Loss: 0.5017\n",
      "Validation Loss: 0.4922, Accuracy: 86.36%\n",
      "Epoch [7/20], Loss: 0.4936\n",
      "Validation Loss: 0.4799, Accuracy: 86.36%\n",
      "Epoch [8/20], Loss: 0.4733\n",
      "Validation Loss: 0.4700, Accuracy: 88.64%\n",
      "Epoch [9/20], Loss: 0.4691\n",
      "Validation Loss: 0.4629, Accuracy: 88.64%\n",
      "Epoch [10/20], Loss: 0.4615\n",
      "Validation Loss: 0.4570, Accuracy: 88.64%\n",
      "Epoch [11/20], Loss: 0.4488\n",
      "Validation Loss: 0.4505, Accuracy: 86.36%\n",
      "Epoch [12/20], Loss: 0.4440\n",
      "Validation Loss: 0.4468, Accuracy: 86.36%\n",
      "Epoch [13/20], Loss: 0.4413\n",
      "Validation Loss: 0.4428, Accuracy: 86.36%\n",
      "Epoch [14/20], Loss: 0.4297\n",
      "Validation Loss: 0.4404, Accuracy: 86.36%\n",
      "Epoch [15/20], Loss: 0.4356\n",
      "Validation Loss: 0.4376, Accuracy: 86.36%\n",
      "Epoch [16/20], Loss: 0.4242\n",
      "Validation Loss: 0.4348, Accuracy: 86.36%\n",
      "Epoch [17/20], Loss: 0.4306\n",
      "Validation Loss: 0.4324, Accuracy: 86.36%\n",
      "Epoch [18/20], Loss: 0.4194\n",
      "Validation Loss: 0.4297, Accuracy: 88.64%\n",
      "Epoch [19/20], Loss: 0.4169\n",
      "Validation Loss: 0.4275, Accuracy: 88.64%\n",
      "Epoch [20/20], Loss: 0.4119\n",
      "Validation Loss: 0.4257, Accuracy: 88.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/03 22:02:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and logged in MLflow.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(nested=True):\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"input_size\", input_size)\n",
    "    mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "    mlflow.log_param(\"output_size\", output_size)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "\n",
    "    mlflow.log_param(\"model_name\", 'neural network')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    print(\"Model training complete and logged in MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554a81c-9c69-4c94-9923-3224c7cf0904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72c8ded4-2350-42c8-aa32-d0e2f977083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Pass the input tensor to the model\n",
    "with torch.no_grad(): \n",
    "    predictions = model(X_test)\n",
    "\n",
    "y_pred = torch.argmax(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c81649e-c034-4878-90c9-6f467566a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.7142857313156128\n",
      "Accuracy: 0.9555555582046509\n"
     ]
    }
   ],
   "source": [
    "y_true = y_test\n",
    "\n",
    "# Calculate True Positives, False Positives, and False Negatives\n",
    "tp = ((y_true == 1) & (y_pred == 1)).sum()  # True Positives\n",
    "fp = ((y_true == 0) & (y_pred == 1)).sum()  # False Positives\n",
    "fn = ((y_true == 1) & (y_pred == 0)).sum()  # False Negatives\n",
    "tn = ((y_true == 0) & (y_pred == 0)).sum()  # True Negatives\n",
    "\n",
    "# Precision and Recall\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32c765ed-bff1-4b5e-9a3e-5d3bef2d5b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f358f8e8-41c5-4ad6-a1f9-03e6084cee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correctly predicted as MVP\n",
      "1984-85 LARRY BIRD\n",
      "1994-95 DAVID ROBINSON\n",
      "2003-04 KEVIN GARNETT\n",
      "2011-12 LEBRON JAMES\n",
      "1999-00 SHAQUILLE O'NEAL\n",
      "\n",
      "Incorrectly predicted as MVP\n",
      "\n",
      "Incorrectly predicted as non-MVP\n",
      "2022-23 JOEL EMBIID\n",
      "2001-02 TIM DUNCAN\n",
      "\n",
      "Correctly predicted as non-MVP\n",
      "1984-85 MAGIC JOHNSON\n",
      "1984-85 MOSES MALONE\n",
      "1984-85 TERRY CUMMINGS\n",
      "1994-95 CHARLES BARKLEY\n",
      "1994-95 HAKEEM OLAJUWON\n",
      "1994-95 KARL MALONE\n",
      "1994-95 PATRICK EWING\n",
      "1994-95 SCOTTIE PIPPEN\n",
      "1994-95 SHAQUILLE O'NEAL\n",
      "2003-04 BEN WALLACE\n",
      "2003-04 JERMAINE O'NEAL\n",
      "2003-04 KOBE BRYANT\n",
      "2003-04 SHAQUILLE O'NEAL\n",
      "2003-04 TIM DUNCAN\n",
      "2011-12 CHRIS PAUL\n",
      "2011-12 DWIGHT HOWARD\n",
      "2011-12 KEVIN DURANT\n",
      "2011-12 KEVIN LOVE\n",
      "2011-12 KOBE BRYANT\n",
      "2011-12 TONY PARKER\n",
      "1999-00 ALLEN IVERSON\n",
      "1999-00 ALONZO MOURNING\n",
      "1999-00 GARY PAYTON\n",
      "1999-00 KARL MALONE\n",
      "1999-00 KEVIN GARNETT\n",
      "1999-00 TIM DUNCAN\n",
      "2022-23 DOMANTAS SABONIS\n",
      "2022-23 DONOVAN MITCHELL\n",
      "2022-23 GIANNIS ANTETOKOUNMPO\n",
      "2022-23 JAYSON TATUM\n",
      "2022-23 NIKOLA JOKIĆ\n",
      "2022-23 SHAI GILGEOUS-ALEXANDER\n",
      "2001-02 CHRIS WEBBER\n",
      "2001-02 GARY PAYTON\n",
      "2001-02 JASON KIDD\n",
      "2001-02 KOBE BRYANT\n",
      "2001-02 SHAQUILLE O'NEAL\n",
      "2001-02 TRACY MCGRADY\n"
     ]
    }
   ],
   "source": [
    "# subset_indexes = _y_test.index\n",
    "\n",
    "y_pred_np = y_pred.numpy()\n",
    "\n",
    "true_positive = _y_test.reset_index().loc[(_y_test.reset_index()['mvp'] == 1) & (y_pred_np == 1)]\n",
    "false_positive = _y_test.reset_index().loc[(_y_test.reset_index()['mvp'] == 0) & (y_pred_np == 1)]\n",
    "false_negative = _y_test.reset_index().loc[(_y_test.reset_index()['mvp'] == 1) & (y_pred_np == 0)]\n",
    "true_negative = _y_test.reset_index().loc[(_y_test.reset_index()['mvp'] == 0) & (y_pred_np == 0)]\n",
    "\n",
    "# print(f\"true_positive:\\n {true_positive}\")\n",
    "# print(f\"false_positive:\\n {false_positive}\")\n",
    "# print(f\"false_negative:\\n {false_negative}\")\n",
    "\n",
    "cross_refs_path = \"/Users/cb/src/nba_mvp_ml/data/processed/by_season/fully_merged/player_index_mapping.csv\"\n",
    "cross_refs = pd.read_csv(cross_refs_path)\n",
    "\n",
    "print('\\nCorrectly predicted as MVP')\n",
    "for i in list(true_positive['index']):\n",
    "    player = cross_refs.iloc[i]['Player']\n",
    "    season = write_season(int(cross_refs.iloc[i]['SEASON_ID']))\n",
    "    \n",
    "    print(f'{season} {player}')\n",
    "\n",
    "print('\\nIncorrectly predicted as MVP')\n",
    "for i in list(false_positive['index']):\n",
    "    player = cross_refs.iloc[i]['Player']\n",
    "    season = write_season(int(cross_refs.iloc[i]['SEASON_ID']))\n",
    "    \n",
    "    print(f'{season} {player}')\n",
    "\n",
    "print('\\nIncorrectly predicted as non-MVP')\n",
    "for i in list(false_negative['index']):\n",
    "    player = cross_refs.iloc[i]['Player']\n",
    "    season = write_season(int(cross_refs.iloc[i]['SEASON_ID']))\n",
    "    \n",
    "    print(f'{season} {player}')\n",
    "\n",
    "print('\\nCorrectly predicted as non-MVP')\n",
    "for i in list(true_negative['index']):\n",
    "    player = cross_refs.iloc[i]['Player']\n",
    "    season = write_season(int(cross_refs.iloc[i]['SEASON_ID']))\n",
    "    \n",
    "    print(f'{season} {player}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9196fe-ecd6-4c2b-b059-76ba6c03a937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd4a46d7-3d5c-4c32-b4f2-e1b6b435e430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb2db1-dfef-42fa-bc55-ba50a680a246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
