{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db3e460-3e56-4316-b92a-4cc32b184152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Adjust to your project's structure\n",
    "sys.path.append(project_root)\n",
    "from src.analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bf2226-72bf-45f0-b613-20a4cb1226ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLFlow tracking URI (local or server-based)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")  # Change if using a centralized server\n",
    "\n",
    "# Define the experiment name\n",
    "mlflow.set_experiment(\"MVP Prediction (NN)\")\n",
    "\n",
    "mlflow.set_tag(\"developer\", \"cbrown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01634e9-f934-45a5-8f61-5c4fbf077394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: torch.Size([206, 24]) torch.Size([206])\n",
      "Validation set: torch.Size([44, 24]) torch.Size([44])\n",
      "Test set: torch.Size([45, 24]) torch.Size([45])\n"
     ]
    }
   ],
   "source": [
    "# Load your cleaned dataset\n",
    "data_path = \"/Users/cb/src/nba_mvp_ml/data/processed/by_season/fully_merged/final_stacked_data.csv\"\n",
    "\n",
    "_X, _y = load_and_preprocess_data(data_path, remove_excess_features=True) # X will be normalized\n",
    "\n",
    "\n",
    "# Example input data\n",
    "np.random.seed(42)\n",
    "X =_X.to_numpy().astype(np.float32)\n",
    "y = _y.to_numpy().astype(np.int64)  # Binary labels\n",
    "\n",
    "# Determine sizes for train, validation, and test splits\n",
    "train_size = int(0.7 * len(X))  # 70% for training\n",
    "val_size = int(0.15 * len(X))   # 15% for validation\n",
    "test_size = len(X) - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "# Split the datase\n",
    "X_train = torch.tensor(X[:train_size])\n",
    "y_train = torch.tensor(y[:train_size])\n",
    "\n",
    "X_val= torch.tensor(X[train_size:train_size + val_size])\n",
    "y_val= torch.tensor(y[train_size:train_size + val_size])\n",
    "\n",
    "X_test = torch.tensor(X[train_size + val_size:])\n",
    "y_test = torch.tensor(y[train_size + val_size:])\n",
    "\n",
    "_y_test = _y[train_size + val_size:]\n",
    "\n",
    "# Check the shapes of each split\n",
    "print(\"Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b29ec2-4008-42f1-b19c-727b1b40dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_pct=0):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01) \n",
    "        self.dropout = nn.Dropout(p=dropout_pct)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.bn1(x)  # Batch normalization\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523dc07a-46c1-4c8d-a9fb-bf657dfd9d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 20/20 [01:52<00:00,  5.62s/trial, best loss: -0.9545454545454546]\n",
      "Best hyperparameters: {'batch_size': 36.0, 'dropout_pct': 26.0, 'hidden_size': 48.0, 'learning_rate': 0.0033595262605951083, 'num_epochs': 80.0}\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective(params):\n",
    "    input_size = len(_X.columns)\n",
    "    hidden_size = int(params[\"hidden_size\"])\n",
    "    output_size = 2\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    batch_size = int(params[\"batch_size\"])\n",
    "    num_epochs = int(params[\"num_epochs\"])\n",
    "    dropout_pct = int(params[\"dropout_pct\"])\n",
    "    \n",
    "    # Update DataLoaders with new batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = SimpleMLP(input_size, hidden_size, output_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(val_loader) \n",
    "    \n",
    "    # Log parameters and metrics to MLflow\n",
    "    with mlflow.start_run(run_name=f\"{X.shape[1]} features\", nested=True):\n",
    "        y_pred = torch.argmax(model(X_test), dim=1).numpy()\n",
    "\n",
    "        y_true = y_test\n",
    "\n",
    "        # Calculate True Positives, False Positives, and False Negatives\n",
    "        tp = ((y_true == 1) & (y_pred == 1)).sum()  # True Positives\n",
    "        fp = ((y_true == 0) & (y_pred == 1)).sum()  # False Positives\n",
    "        fn = ((y_true == 1) & (y_pred == 0)).sum()  # False Negatives\n",
    "        tn = ((y_true == 0) & (y_pred == 0)).sum()  # True Negatives\n",
    "        \n",
    "        # Precision and Recall\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"val_loss\", avg_val_loss)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            signature=infer_signature(X_test.numpy(), y_pred),\n",
    "            artifact_path=\"nn-model\",\n",
    "        )\n",
    "    \n",
    "    # Return validation loss for Hyperopt to minimize\n",
    "    return {\"loss\": -val_accuracy, \"status\": STATUS_OK}\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    \"hidden_size\": hp.quniform(\"hidden_size\", 16, 128, 16),  # Discrete range\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -6, -1),  # Log scale\n",
    "    \"batch_size\": hp.quniform(\"batch_size\", 4, 64, 4),\n",
    "    \"num_epochs\": hp.quniform(\"num_epochs\", 20, 500, 20),\n",
    "    \"dropout_pct\": hp.quniform(\"dropout_pct\", 0, 70, 1)\n",
    "}\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,  # Number of evaluations\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af081feb-ef14-4504-87c8-733d4cfd8260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FGM',\n",
       " 'BPM',\n",
       " 'DRBPct',\n",
       " 'DWS',\n",
       " 'OBPM',\n",
       " 'PER',\n",
       " 'TOVPct',\n",
       " 'VORP',\n",
       " 'WS/48_x',\n",
       " 'Rk_opp_pg',\n",
       " '2P%_opp_pg',\n",
       " 'DRB_opp_pg',\n",
       " 'SRS',\n",
       " 'ORtg',\n",
       " 'sentiment_1',\n",
       " 'sentiment_2',\n",
       " 'sentiment_3',\n",
       " 'sentiment_5',\n",
       " 'sentiment_6',\n",
       " 'sentiment_8',\n",
       " 'sentiment_13',\n",
       " 'sentiment_14',\n",
       " 'sentiment_avg',\n",
       " 'WS']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdd1c3-64e9-4588-a5d3-f6d4dbc2d6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d6ddc-0a43-40fa-8a91-529391ababad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b5e53-344d-43ca-9558-29240401c0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "758e203f-d8f2-4611-a09b-1bf8febdb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #maybe do k fold?\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "\n",
    "# # Number of folds for cross-validation\n",
    "# k_folds = 5\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# # Prepare the data\n",
    "# X = _X.to_numpy().astype(np.float32)\n",
    "# y = _y.to_numpy().astype(np.int64)\n",
    "\n",
    "# # Hyperparameters\n",
    "# input_size = len(_X.columns)\n",
    "# hidden_size = 64\n",
    "# output_size = 2\n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 100\n",
    "# batch_size = 32\n",
    "\n",
    "# # Metrics to track across folds\n",
    "# fold_results = []\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "#     print(f\"\\n--- Fold {fold + 1}/{k_folds} ---\")\n",
    "    \n",
    "#     # Split data into training and validation sets for this fold\n",
    "#     X_train, y_train = torch.tensor(X[train_idx]), torch.tensor(y[train_idx])\n",
    "#     X_val, y_val = torch.tensor(X[val_idx]), torch.tensor(y[val_idx])\n",
    "    \n",
    "#     train_dataset = TensorDataset(X_train, y_train)\n",
    "#     val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     # Initialize the model, optimizer, and loss function for this fold\n",
    "#     model = SimpleMLP(input_size, hidden_size, output_size).to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         for inputs, targets in train_loader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "#         # Validation loop\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in val_loader:\n",
    "#                 inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, targets)\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 total += targets.size(0)\n",
    "#                 correct += (predicted == targets).sum().item()\n",
    "\n",
    "#         avg_val_loss = val_loss / len(val_loader)\n",
    "#         val_accuracy = 100 * correct / total\n",
    "\n",
    "#         if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "#             print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "#     # Log fold results\n",
    "#     fold_results.append({\n",
    "#         \"fold\": fold + 1,\n",
    "#         \"val_loss\": avg_val_loss,\n",
    "#         \"val_accuracy\": val_accuracy\n",
    "#     })\n",
    "\n",
    "#     # Log metrics to MLFlow\n",
    "#     with mlflow.start_run(nested=True):\n",
    "#         mlflow.log_param(\"fold\", fold + 1)\n",
    "#         mlflow.log_param(\"input_size\", input_size)\n",
    "#         mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "#         mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "#         mlflow.log_metric(\"val_loss\", avg_val_loss)\n",
    "#         mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "\n",
    "# # Summarize cross-validation results\n",
    "# mean_val_accuracy = np.mean([result[\"val_accuracy\"] for result in fold_results])\n",
    "# std_val_accuracy = np.std([result[\"val_accuracy\"] for result in fold_results])\n",
    "\n",
    "# print(\"\\n--- Cross-Validation Results ---\")\n",
    "# for result in fold_results:\n",
    "#     print(f\"Fold {result['fold']}: Val Loss = {result['val_loss']:.4f}, Val Accuracy = {result['val_accuracy']:.2f}%\")\n",
    "# print(f\"Mean Val Accuracy: {mean_val_accuracy:.2f}%, Std Val Accuracy: {std_val_accuracy:.2f}%\")\n",
    "\n",
    "# # Log overall cross-validation results\n",
    "# with mlflow.start_run(nested=True):\n",
    "#     mlflow.log_metric(\"mean_val_accuracy\", mean_val_accuracy)\n",
    "#     mlflow.log_metric(\"std_val_accuracy\", std_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accebf86-341e-46c7-b5af-fa20346fd225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897423bb-7d99-4681-88f6-32cb7360ee67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
